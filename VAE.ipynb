{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3d43e3f-20ad-49cf-8979-8a4127eb4f9e",
   "metadata": {},
   "source": [
    "## VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43157706-133c-434f-9c7c-9bfc77ed70e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import (\n",
    "    Collector,\n",
    "    VAELoss,\n",
    "    trainer,\n",
    "    plot_latent_tsne,\n",
    "    generate_samples_between_centers,\n",
    "    visualize_prediction,\n",
    ")\n",
    "\n",
    "from models import (\n",
    "    deconv_resnet18,\n",
    "    resnet18,\n",
    ")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725c89d4-9672-4719-989b-0d8fc66686b0",
   "metadata": {},
   "source": [
    "## Set Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07243923-76e1-4a7f-bd1e-391a89e01c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(seed)\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2e4343-b0b2-442b-84dd-6224b33e336a",
   "metadata": {},
   "source": [
    "## Basic Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c73015e-4739-49ae-96c2-7e21d5b64938",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "epochs = 50\n",
    "bs = 256\n",
    "num_workers = 8\n",
    "mean = [0, 0, 0]\n",
    "std = [1, 1, 1]\n",
    "image_size = [128, 128]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69651889-d9b2-409a-8cf7-5ee5a95207b0",
   "metadata": {},
   "source": [
    "## Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fddd3e-4246-4a2c-a1be-40f96a4af682",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transforms:\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        transforms: A.Compose,\n",
    "    ) -> None:\n",
    "        \n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        img,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ) -> torch.Tensor:\n",
    "        \n",
    "        return self.transforms(\n",
    "            image=np.array(img),\n",
    "            test_image=np.array(img),\n",
    "        )\n",
    "\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset: torch.utils.data.Dataset,\n",
    "        transforms: Transforms,\n",
    "        kwargs,\n",
    "    ) -> None:\n",
    "        \n",
    "        super().__init__()\n",
    "        self.dataset = dataset(**kwargs)\n",
    "        self.transforms = transforms\n",
    "            \n",
    "        \n",
    "    def __getitem__(\n",
    "        self,\n",
    "        idx: int,\n",
    "    ) -> dict:\n",
    "        \n",
    "        images, classes = self.dataset.__getitem__(idx)\n",
    "        \n",
    "        if not isinstance(images, np.ndarray):\n",
    "            images = np.array(images)\n",
    "        \n",
    "        if len(images.shape) == 2:\n",
    "            images = images[:, :, None]\n",
    "        \n",
    "        if images.shape[2] == 1:\n",
    "            images = np.tile(images, (1, 1, 3))\n",
    "            \n",
    "        return {\n",
    "            'images': self.transforms(images),\n",
    "            'class': classes,\n",
    "        }\n",
    "    \n",
    "    def __len__(\n",
    "        self,\n",
    "    ) -> int:\n",
    "        \n",
    "        return len(self.dataset)\n",
    "    \n",
    "in_tf = A.Compose([\n",
    "    A.LongestMaxSize(max_size=max(image_size)),\n",
    "    A.PadIfNeeded(\n",
    "        position=A.PadIfNeeded.PositionType.TOP_LEFT,\n",
    "        min_height=image_size[0],\n",
    "        min_width=image_size[1],\n",
    "        value=0,\n",
    "        border_mode=cv2.BORDER_CONSTANT,\n",
    "    ),\n",
    "], additional_targets = {\n",
    "    'test_image': 'image',\n",
    "})\n",
    "\n",
    "middle_tf = A.Compose([\n",
    "    A.CoarseDropout(\n",
    "        max_holes=4,\n",
    "        min_holes=1,\n",
    "        max_height=0.2,\n",
    "        min_height=0.05,\n",
    "        max_width=0.2,\n",
    "        min_width=0.05,\n",
    "        fill_value=[0, 0.5, 1],\n",
    "        p=0.5,\n",
    "    ),\n",
    "    A.OneOf([\n",
    "        A.RandomBrightnessContrast(\n",
    "            brightness_limit=(-0.5, 0.5),\n",
    "            contrast_limit=(-0.5, 0.5),\n",
    "            p=1,\n",
    "        ),\n",
    "        A.Blur(\n",
    "            p=1,\n",
    "        ),\n",
    "        A.GaussNoise(\n",
    "            var_limit=5.0 / 255.0,\n",
    "            p=1,\n",
    "        ),\n",
    "    ], p=1)\n",
    "])\n",
    "\n",
    "out_tf = A.Compose([\n",
    "    A.Normalize(\n",
    "        mean=mean,\n",
    "        std=std,\n",
    "    ),\n",
    "    ToTensorV2(),\n",
    "], additional_targets = {\n",
    "    'test_image': 'image',\n",
    "})\n",
    "    \n",
    "transformations = {\n",
    "    'train': A.Compose([\n",
    "        in_tf,\n",
    "        middle_tf,\n",
    "        out_tf,\n",
    "    ]),\n",
    "    'test': A.Compose([\n",
    "        in_tf,\n",
    "        out_tf,\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# STL10\n",
    "stl10 = {\n",
    "    'train': {\n",
    "        'root': './data/',\n",
    "        'split': 'unlabeled',\n",
    "        'download': True,\n",
    "        'transform': Transforms(transformations['train']),\n",
    "    },\n",
    "    'test': {\n",
    "        'root': './data/',\n",
    "        'split': 'test',\n",
    "        'download': True,\n",
    "        'transform': Transforms(transformations['test']),\n",
    "    },\n",
    "}\n",
    "\n",
    "# MNIST\n",
    "mnist = {\n",
    "    'train': {\n",
    "        'root': './data/',\n",
    "        'train': True,\n",
    "        'download': True,\n",
    "    },\n",
    "    'test': {\n",
    "        'root': './data/',\n",
    "        'train': False,\n",
    "        'download': True,\n",
    "    },\n",
    "}\n",
    "\n",
    "dataset = {\n",
    "    'mnist': {\n",
    "        'phase': mnist,\n",
    "        'class': torchvision.datasets.MNIST,\n",
    "    },\n",
    "    'stl10': {\n",
    "        'phase': stl10,\n",
    "        'class': torchvision.datasets.STL10,\n",
    "    },\n",
    "}\n",
    "\n",
    "dataset_name = 'mnist'\n",
    "\n",
    "datasets = {\n",
    "    phase: MyDataset(\n",
    "        dataset[dataset_name]['class'],\n",
    "        Transforms(transformations[phase]),\n",
    "        dataset[dataset_name]['phase'][phase]\n",
    "    )\n",
    "    for phase in dataset[dataset_name]['phase']\n",
    "}\n",
    "\n",
    "dataloaders = {\n",
    "    phase: torch.utils.data.DataLoader(\n",
    "        dataset=datasets[phase],\n",
    "        batch_size=bs,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "        worker_init_fn=seed_worker,\n",
    "        generator=g,\n",
    "    )\n",
    "    for phase in dataset[dataset_name]['phase']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56537967-a9c3-4de1-8471-af1aa2a19f42",
   "metadata": {},
   "source": [
    "## Let's visualize the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cd8e5a-4abf-4c1e-90b1-4cb4909061ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_prediction(\n",
    "    model=None,\n",
    "    dataset=datasets['train'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd5294b-f454-4fd9-8b53-176fdf14eaf9",
   "metadata": {},
   "source": [
    "## Define model\n",
    "![alt text](pics/vae.png \"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93554ba-61a0-49e3-b406-d124578c1d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "class UnFlatten(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        b, c = x.shape\n",
    "        return x.view(b, c, 1, 1)\n",
    "\n",
    "class VariationalAE(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        downlayers: nn.Module,\n",
    "        uplayers: nn.Module,\n",
    "        hidden_dim: int=512,\n",
    "        latent_dim: int=512,\n",
    "    ) -> None:\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.down = downlayers()\n",
    "        self.up = uplayers()\n",
    "        \n",
    "        self.agg = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.middle = nn.ConvTranspose2d(\n",
    "            in_channels=latent_dim,\n",
    "            out_channels=hidden_dim,\n",
    "            kernel_size=(image_size[0] // 32, image_size[1] // 32),\n",
    "            stride=2,\n",
    "        )\n",
    "        \n",
    "        self.mu_repr = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.log_sigma_repr = nn.Linear(hidden_dim, latent_dim)\n",
    "        \n",
    "        self.flatten = Flatten()\n",
    "        self.unflatten = UnFlatten()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        # hack to get sampling on the GPU\n",
    "        self.samling = torch.distributions.Normal(0, 1)\n",
    "        self.samling.loc = self.samling.loc.to(device)\n",
    "        self.samling.scale = self.samling.scale.to(device)\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(\n",
    "        self,\n",
    "    ) -> None:\n",
    "        \n",
    "        for m in self.modules():\n",
    "            \n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(\n",
    "                    tensor=m.weight,\n",
    "                )\n",
    "                nn.init.zeros_(\n",
    "                    tensor=m.bias,\n",
    "                )\n",
    "        \n",
    "    def _encode(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \n",
    "        x = self.down(x)\n",
    "        x = self.agg(x)\n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def _decode(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \n",
    "        x = self.unflatten(x)\n",
    "        x = self.middle(x)\n",
    "        x = self.up(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def _reparametrize(\n",
    "        self,\n",
    "        mu: torch.Tensor,\n",
    "        log_sigma: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \n",
    "        eps = self.samling.sample(mu.shape)\n",
    "        x = eps * log_sigma.exp() + mu\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        embedding: bool=True,\n",
    "    ) -> torch.Tensor:\n",
    "        \n",
    "        latent_repr = self._encode(x)\n",
    "        \n",
    "        if embedding:\n",
    "            return latent_repr\n",
    "        \n",
    "        latent_mu = self.mu_repr(latent_repr)\n",
    "        latent_log_sigma = self.log_sigma_repr(latent_repr)\n",
    "        \n",
    "        sample = self._reparametrize(latent_mu, latent_log_sigma)\n",
    "        \n",
    "        image = self._decode(sample)\n",
    "        \n",
    "        return {\n",
    "            'pred_image': image,\n",
    "            'mu': latent_mu,\n",
    "            'log_sigma': latent_log_sigma,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24933c5-89af-4727-8838-dd374760182c",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim=1024\n",
    "\n",
    "model = VariationalAE(\n",
    "    downlayers=resnet18,\n",
    "    uplayers=deconv_resnet18,\n",
    "    latent_dim=latent_dim,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fdb35a-aa2a-4525-8181-e2bc2aa0edb6",
   "metadata": {},
   "source": [
    "## Define trainer params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302157aa-6baa-4615-b7a4-67223198cb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=5e-4\n",
    "weights=[1,100]\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    params=model.parameters(),\n",
    "    lr=lr,\n",
    "    weight_decay=1e-3,\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer=optimizer,\n",
    "    max_lr=lr,\n",
    "    steps_per_epoch=len(dataloaders['train']),\n",
    "    epochs=epochs,\n",
    ")\n",
    "loss = VAELoss(weights=weights)\n",
    "\n",
    "save_path=f'metrics/VAE/dataset={dataset_name}_epoch={epochs}_bs={bs}_lr={lr}_loss={loss.__class__.__name__}_weights={weights}_latent={latent_dim}_norm=max'\n",
    "visualiser = Collector(\n",
    "    root_graphics=save_path,\n",
    "    root_desc=save_path,\n",
    "    phases=list(transformations.keys()),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33ccb28-a1af-4dc1-84ee-bacb3ff1155f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer(model, optimizer, scheduler, dataloaders, epochs, device, loss, visualiser, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428ca784-0c56-4495-9a8e-ddc2a2ab68fa",
   "metadata": {},
   "source": [
    "## Visualize predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0eef06-4a87-41db-bd19-24c8ce50fbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_latent_tsne(model, dataloaders['test'], save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9503758-14ba-4979-8fe2-2870d32aaf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def visualize_prediction(\n",
    "    model: nn.Module,\n",
    "    dataset: torch.utils.data.Dataset,\n",
    "    save_path: str=None,\n",
    "    n_samples: int=10,\n",
    "    device: torch.device='cuda',\n",
    ") -> None:\n",
    "    \n",
    "    fig, axes = plt.subplots(n_samples, 3, figsize=(10, 40))\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        index = random.randint(0, len(dataset))\n",
    "            \n",
    "        sample = dataset[index]\n",
    "        \n",
    "        inputs = sample['images']['image']\n",
    "        outputs = sample['images']['test_image']\n",
    "        \n",
    "        if model is not None:\n",
    "            model.eval()\n",
    "            preds = model(\n",
    "                x=inputs.to(next(model.parameters()).device).unsqueeze(0),\n",
    "                embedding=False,\n",
    "            )['pred_image'].squeeze(0).sigmoid()\n",
    "        else:\n",
    "            preds = torch.zeros(inputs.shape)\n",
    "            \n",
    "        for j, (data, title) in enumerate(zip([inputs, outputs, preds], ['input', 'output', 'pred'])):            \n",
    "            axes[i][j].imshow(data.permute(1, 2, 0).cpu().numpy())\n",
    "            axes[i][j].set_title(title)\n",
    "    \n",
    "    if save_path is not None:\n",
    "        plt.savefig(os.path.join(save_path, 'predictions.jpg'), format='jpg')\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c074bf86-b9f8-48e7-a337-c01aaf56d719",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_prediction(model, datasets['test'], save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71686aca-446d-4f71-8bed-54d823665e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples_between_centers(\n",
    "    model: nn.Module,\n",
    "    dataloader: torch.utils.data.DataLoader,\n",
    "    save_path: str=None,\n",
    "    from_class: int=9,\n",
    "    to_class: int=7,\n",
    "    rows_cols: int=2,\n",
    "    device: torch.device='cuda',\n",
    ") -> None:\n",
    "    \n",
    "    centroids = []\n",
    "    classes = []\n",
    "    \n",
    "    for gt_batch in dataloader:\n",
    "        preds_batch = model(\n",
    "            x=gt_batch['images']['test_image'].to(next(model.parameters()).device),\n",
    "            embedding=False,\n",
    "        )\n",
    "        \n",
    "        centroids.append(preds_batch['mu'].detach().cpu().numpy())\n",
    "        classes.append(gt_batch['class'].numpy())\n",
    "        \n",
    "    centroids = np.concatenate(centroids)\n",
    "    classes = np.concatenate(classes)\n",
    "    \n",
    "    center_from = centroids[classes == from_class].mean(axis=0)\n",
    "    center_to = centroids[classes == to_class].mean(axis=0)\n",
    "    \n",
    "    z = torch.stack([\n",
    "        torch.from_numpy(t*center_from + (1 - t) * center_to)\n",
    "        for t in np.linspace(0, 1, rows_cols*rows_cols)\n",
    "    ])\n",
    "    \n",
    "    images = model._decode(z.to(device)).permute(0, 2, 3, 1).cpu().sigmoid().detach().numpy()\n",
    "    \n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "    for i, img in enumerate(images):\n",
    "        fig.add_subplot(rows_cols, rows_cols, i + 1)\n",
    "        plt.imshow(img)\n",
    "    \n",
    "    if save_path is not None:\n",
    "        plt.savefig(os.path.join(save_path, 'samples_between_centers.jpg'), format='jpg')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb55299-e324-45bb-911f-37ffe69b62bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_samples_between_centers(model, dataloaders['test'], save_path, from_class=9, to_class=1, rows_cols=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463bd30f-07ec-4475-be15-05bb52453239",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_embeddings(\n",
    "    path: str,\n",
    "    transforms,\n",
    ") -> torch.Tensor:\n",
    "    \n",
    "    batch = torch.load(path).mul_(255).permute(0, 2, 3, 1).numpy().astype(np.uint8)\n",
    "    new_batch = []\n",
    "    \n",
    "    for image in batch:\n",
    "        new_batch.append(\n",
    "            transforms(image=image)['image'],\n",
    "        )\n",
    "        \n",
    "    new_batch = torch.stack(new_batch)\n",
    "    \n",
    "    return new_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa6bf6f-3b95-46ba-ae0b-9e30adb2f2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "traced_module = torch.jit.trace(model.cpu(), torch.randn(1, 3, image_size[0], image_size[1]))\n",
    "torch.jit.save(traced_module, os.path.join(save_path, 'model_traced.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da77e39-766b-4f4c-bfcc-27ebedd8a13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded = torch.jit.load(os.path.join(save_path, 'model_traced.pt'))\n",
    "batch = convert_embeddings('health_dataset.pth', transformations['test'])\n",
    "\n",
    "with torch.no_grad():\n",
    "    embeddings = loaded(batch.cpu()).cpu()\n",
    "    \n",
    "torch.save(embeddings, os.path.join(save_path, 'embeddings.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c130f3d-a642-4493-8f2b-0d586b3874c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
